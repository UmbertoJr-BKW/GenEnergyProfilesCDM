{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6db7550-523c-450b-9373-438987acd9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 07:12:29.408778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "\n",
    "from auto_encoder import EncoderLSTM, DecoderLSTM\n",
    "from data_handler import load_sm_data\n",
    "from trainer import train_AutoEncoder\n",
    "from utils import restore_AE_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ee56c0-6823-4d63-87d4-2875bc455e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Option:\n",
    "    module = \"LSTM\"\n",
    "    num_layer = 2 #3\n",
    "    hidden_dim = int(32**2/2) #32\n",
    "    batch_size = 32\n",
    "    norm = False\n",
    "    alpha_norm = 1e-4\n",
    "    z_dim = 1\n",
    "    seq_len = 96\n",
    "    data_name = \"sm_consumption\"\n",
    "    lr = 0.001\n",
    "    beta1 = 0.9\n",
    "    epoch = 200\n",
    "    size_data = 1974\n",
    "    n_epochs = 500\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "opt = Option()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d9ff2b-2bf6-449e-be25-6df9178668fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_consumption dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "ori_data = load_sm_data(opt)\n",
    "dataset = np.stack(ori_data).reshape(-1, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d826b4dd-b92a-48bc-9ef0-7585120176aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load scaler_params  \n",
    "with open(\"scaler_params.pkl\", \"rb\") as f:  \n",
    "    scaler_params = pickle.load(f)  \n",
    "\n",
    "    \n",
    "# Let's say you want to invert the scaling for the first row  \n",
    "row = 0  \n",
    "data_min, data_max = scaler_params[row]  \n",
    "scaled_values = dataset[row, :]  \n",
    "original_values = scaled_values * (data_max - data_min) + data_min  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5769fc-f9be-4473-afdf-39d2656d815e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AE_trained/checkpoints/LSTM/2-l/512-Hdim/32-bs/n-norm/checkpoint_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_13417/2124805109.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  input_data = torch.tensor(ori_data, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "ER_dir = f\"./AE_trained/checkpoints/{opt.module}/{opt.num_layer}-l/{opt.hidden_dim}-Hdim/{opt.batch_size}-bs{['/n-norm',f'/y-norm/{opt.alpha_norm}-an'][opt.norm]}\"\n",
    "ckpt = sorted([ int(i[11:-4]) for i in os.listdir(ER_dir)])[-1]\n",
    "ckpt_dir = os.path.join(ER_dir, f\"checkpoint_{ckpt}.pth\")\n",
    "print(ckpt_dir)\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "nete = EncoderLSTM(input_size=opt.z_dim, \n",
    "                       hidden_dim=opt.hidden_dim, \n",
    "                       batch_size=opt.batch_size, \n",
    "                       n_layers=opt.num_layer,\n",
    "                       device=device).to(device)\n",
    "    \n",
    "netr = DecoderLSTM(hidden_dim=opt.hidden_dim,\n",
    "                   output_size=opt.z_dim,\n",
    "                   batch_size=opt.batch_size,\n",
    "                   n_layers=opt.num_layer,\n",
    "                   forecasting_horizon=opt.seq_len,\n",
    "                   device=device).to(device)\n",
    "\n",
    "state_ER = {'encoder': nete, 'decoder': netr}\n",
    "state_ER = restore_AE_checkpoint(ckpt_dir, state_ER, device)\n",
    "\n",
    "input_data = torch.tensor(ori_data, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "encoder = state_ER['encoder']\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "decoder = state_ER['decoder']\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(batch_size=input_data.shape[0])\n",
    "    output_encoder, hidden_embeddings = encoder.forward(input_data, encoder_hidden)\n",
    "    input_decoder = hidden_embeddings[0][-1, :, :].unsqueeze(0).permute(1, 0, 2)\n",
    "    reconstructed_data, decoder_hidden, _ = decoder.forward(decoder_input=input_decoder, encoder_hidden=hidden_embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e12f2-e2ad-499a-b00c-22733f81bc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_time_series(time_series_tensor,reconstructed, i):  \n",
    "    plt.figure(figsize=(14,6))  \n",
    "    plt.plot(time_series_tensor[i,:,0])  \n",
    "    plt.plot(reconstructed[i,:,0], c=\"r\")  \n",
    "    plt.title(\"Time Series Visualization\")  \n",
    "    plt.xlabel(\"Time steps\")  \n",
    "    plt.ylabel(\"Value\")  \n",
    "    plt.show()  \n",
    "\n",
    "i = 2\n",
    "# To use it  \n",
    "visualize_time_series(input_data, reconstructed_data, i) \n",
    "visualize_time_series(input_data, reconstructed_data, 200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf19fd-c252-4d11-8901-887e786552f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_embs = []\n",
    "for hidden_emb in hidden_embeddings:\n",
    "    # Normalize the tensor  \n",
    "    mean = hidden_emb.mean()  \n",
    "    std = hidden_emb.std()  \n",
    "    normalized = (hidden_emb - mean) / std  \n",
    "    normalized_embs.append(normalized)\n",
    "\n",
    "\n",
    "# Concatenate the tensors along the last dimension  \n",
    "combined = torch.cat(normalized_embs, dim=2) # Shape: [2, 1974, 64]  \n",
    "\n",
    "# Function to visualize a tensor as an image  \n",
    "def visualize_tensor_as_image(tensor, index, i):  \n",
    "    image_data = tensor[i, index, :].reshape(1, 32, 32).permute(1, 2, 0)  \n",
    "    plt.imshow(image_data)  \n",
    "    plt.show()  \n",
    "  \n",
    "# Visualize the first image  \n",
    "for dim_ in range(2):\n",
    "    visualize_tensor_as_image(combined, 2, dim_)\n",
    "    visualize_tensor_as_image(combined, 200, dim_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58628599-7fa6-42a2-b131-b4a93f51a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact  \n",
    "  \n",
    "def visualize(i):  \n",
    "    #visualize_tensor_as_image(combined, i)\n",
    "    visualize_time_series(input_data, reconstructed_data, i)  \n",
    "    \n",
    "  \n",
    "interact(visualize, i=(0, input_data.shape[0]-1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c3a7f-713b-44bf-bea2-0b0cf873346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31251f-084c-4fee-87b3-c64e239f72e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb22b1-d2b4-4193-9604-f55da9c08ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382d7b0-61a6-4625-959b-362a3a085e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089e9ca-9b91-4140-b1e8-61711374df00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef637850-192c-4b1a-815a-a915ed2e0ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c4dea-bf14-4a0a-8de7-c0b78fa003d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c14e51-4dec-43b3-86fc-51e662ae8210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35278b22-522a-497f-9840-f1d728f0beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
